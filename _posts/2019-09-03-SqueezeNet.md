---
layout:     post
title:      SqueezeNet--AlexNet级精度，参数减少50倍，模型大小小于0.5MB
subtitle:   SqueezeNet--AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size
date:       2019-09-03
author:     CJR
header-img: img/2019-09-03-SqueezeNet/post-bg.jpg
catalog: true
mathjax: true
tags:
    - Lightweight Network
    - SqueezeNet
    - CNN
---

## SqueezeNet

&emsp;SqueezeNet是较早出现的轻量化网络之一，该文章2016年的时候就挂了出来，然后投了ICLR 2017，不过被拒了。。。原文可见[SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](https://arxiv.org/abs/1602.07360)。

---

## 摘要

&emsp;最近对深度卷积神经网络（CNNs）的研究主要集中在提高精度上。对于给定的精度级别，通常可以找到多个达到该精度级别的CNN架构。同样的精确度，较小的CNN架构至少提供了三个优势：（1）较小的CNN在分布式训练中需要较少的跨服务器通信。（2）较小的CNN需要更少的带宽将新模型从云端导出到自动驾驶汽车上。（3）较小的CNN更适合部署在FPGAs等内存有限的硬件上。为了提供所有这些优势，我们提出了一个小型CNN架构SqueezeNet。SqueezeNet以少50倍的参数在ImageNet上实现了AlexNet级的精度。此外，通过模型压缩技术，我们可以将SqueezeNet压缩到小于0.5MB（比AlexNet小510倍）。

>SqueezeNet架构可以在这里下载：<https://github.com/DeepScale/SqueezeNet>

## 1. 引言与动机

&emsp;最近对深度卷积神经网络（CNNs）的研究主要集中在提高计算机视觉数据集的准确性。对于给定的精度级别，通常存在多个CNN架构来实现该精度级别。在精度相当的情况下，参数较少的CNN架构具有以下几个优点：

* **更高效的分布式训练.**&emsp;服务器之间的通信是限制分布式CNN训练可扩展性的因素。对于分布式数据并行训练，通信开销与模型中参数的数量成正比。简而言之，小模型训练更快，因为需要更少的通信。

* **减少向客户端导出新模型时的开销.**&emsp;对于自动驾驶，特斯拉等公司会定期将新模型从服务器上复制到客户的汽车上。这种做法通常被称为无线更新。《消费者报告》发现，特斯拉的自动驾驶仪的半自动驾驶功能的安全性随着最近的无线更新而不断提高。然而，今天典型的CNN/DNN模型的无线更新可能需要大量的数据传输。使用AlexNet，这将需要从服务器到汽车的240MB通信。较小的模型需要较少的通信，使得频繁的更新更加可行。

* **可行的FPGA和嵌入式部署.**&emsp;FPGA通常只有不到10MB的片内内存，没有片外内存或存储器。对于推理来说，一个足够小的模型可以直接存储在FPGA上，而不是被内存带宽所限制，同时视频帧可以实时流经FPGA。此外，当在专用集成电路（ASIC）上部署CNN时，一个足够小的模型可以直接存储在芯片上，而较小的模型可以使ASIC适合一个较小的芯片。

&emsp;正如您所看到的，较小的CNN架构有几个优点。考虑到这一点，我们将直接关注发现CNN架构的问题，与已知模型相比，该架构的参数更少，但精度相当。我们发现了这样一种结构，我们称之为SqueezeNet。此外，我们还尝试了一种更有规律的方法来搜索新的CNN架构的设计空间。

&emsp;本文的其余部分组织如下。在第二部分中，我们回顾了相关的工作。然后，在第3和第4节中，我们描述和评价了SqueezeNet体系结构。之后，我们将注意力转向理解CNN架构设计的选择如何影响模型的大小和准确性。通过对类SqueezeNet架构的设计空间的探索，我们得到了这一认识。在第5节中，我们在CNN微架构上进行空间探索设计，我们将其定义为各个层和模块的组织和维数。在第6节中，我们对CNN宏架构进行了空间探索设计，我们将其定义为CNN中的高层层次组织。最后，我们在第7节中总结。简而言之，第3节和第4节对于CNN的研究人员以及那些只想将SqueezeNet应用于新应用程序的实践者都是有用的。其余部分针对的是打算设计自己的CNN架构的高级研究人员。

## 2. 相关工作

### 2.1 模型压缩

&emsp;我们工作的首要目标是找到一个只有很少参数的模型，同时保持准确性。为了解决这个问题，一个明智的方法是使用一个现有的CNN模型，并以有损的方式压缩它。事实上，围绕模型压缩这个主题已经出现了一个研究社区，并且已经报道了几种方法。Denton等人的一种相当直接的方法是将奇异值分解（SVD）应用到一个经过预训练的CNN模型中。Han等人开发了网络剪枝，从一个预训练的模型开始，用0代替阈值以下的参数，形成稀疏矩阵，最后对稀疏CNN进行几次迭代训练。最近，Han等人通过将网络剪枝与量化（小于等于8位）和哈夫曼编码相结合，扩展了它们的工作，以创建一种称为深度压缩的方法，并进一步设计了一种称为EIE的硬件加速器，该加速器直接在压缩模型上运行，实现了大幅加速和节能。

### 2.2 CNN微体系架构

&emsp;卷积在人工神经网络中已经使用了至少25年；LeCun等人在20世纪80年代末推动了CNNs在数字识别应用中的普及。在神经网络中，卷积滤波器通常是三维的，以高度、宽度和通道为关键维度。当应用于图像时，CNN滤波器通常在其第一层中有3个通道（即RGB），并且在随后的每一层$L_{i}$中，滤波器的通道数与$L_{i-1}$层具有的滤波器的通道数相同。LeCun等人的早期工作使用$5\times 5\times Channels$滤波器，而最近的VGG架构广泛使用$3\times 3$滤波器。Network-in-Network和GoogLeNet体系结构家族等模型的部分层使用$1\times 1$滤波器。

>注：这里以及后面的滤波器其实就是指我们常说的卷积

&emsp;随着设计深度CNNs的趋势，为每一层手工选择滤波器尺寸变得很麻烦。为了解决这一问题，提出了由具有特定固定组织的多个卷积层组成的各种更高层次的构建块或模块。例如，GoogLeNet论文提出了Inception模块，它由许多不同尺寸的滤波器组成，通常包括$1\times 1$和$3\times 3$，有时是$5\times 5$，有时是$1\times 3$和$3\times 1$。然后，许多这样的模块被组合在一起，可能还有额外的ad-hoc层，以形成一个完整的网络。我们使用术语CNN微架构来表示各个模块的特定组织和维度。

### 2.3 CNN宏体系架构

&emsp;CNN微体系结构是指单个的层和模块，而我们将CNN宏体系结构定义为多个模块在系统级组织成一个端到端CNN体系结构。

&emsp;也许在最近的文献中，CNN宏观架构研究最广泛的话题是网络深度（即层数）的影响。Simoyan和Zisserman提出了具有12到19层的CNNs的VGG家族，并报道了更深的网络在ImageNet-1k数据集上产生更高的精度。K. He等人提出了更深层次的CNNs，最多可达30层，可以提供更高的ImageNet精度。

&emsp;跨多层或多个模块连接的选择是CNN宏结构研究的一个新兴领域。残差网络（ResNet）和Highway网络都建议使用跨多个层的连接，例如将第3层激活与第6层激活相加。我们将这些连接称为旁路连接。ResNet的作者提供了一个有和没有旁路连接的34层CNN的A/B比较；添加旁路连接可使ImageNet的top-5精度提高2个百分点。

### 2.4 神经网络设计空间探索

&emsp;神经网络（包括深度和卷积神经网络）有很大的设计空间，对于微架构、宏架构、求解器和其他超参数有很多的选择。对于这些因素如何影响神经网络的准确性（即设计空间的形状），社区想要获得直觉似乎是很自然的。神经网络的设计空间探索（DSE）的大部分工作都集中在开发自动化的方法来寻找提供更高精度的神经网络体系结构。这些自动化的DSE方法包括贝叶斯优化、模拟退火、随机搜索和遗传算法。值得赞扬的是，每一篇论文都提供了一个案例，其中提出的DSE方法生成了一个神经网络体系结构，与具有代表性的baseline相比，该体系结构具有更高的精度。然而，这些论文并没有试图对神经网络设计空间的形状提供直观的认识。在本文的后面，我们避开了自动化方法——相反，我们重构CNNs的方式是，我们可以进行有原则的A/B比较，以研究CNN架构决策如何影响模型大小和准确性。

&emsp;在接下来的部分中，我们首先提出并评估进行和不进行模型压缩的SqueezeNet体系结构。然后，我们探讨了微体系结构和宏体系结构中的设计选择对类SqueezeNet CNN体系结构的影响。

## 3. SqueezeNet：少参数保精度

&emsp;在这一部分中，我们首先概述了我们的CNN架构的设计策略，这些架构的参数很少。然后，我们介绍了Fire模块，这是我们构建CNN架构的新模块。最后，利用我们的设计策略构建了以Fire模块为主的SqueezeNet。

### 3.1 架构设计策略

&emsp;在本文中，我们的首要目标是找到具有较少参数的CNN架构，同时保持具有竞争力的准确性。为此，我们在设计CNN架构时采用了三种主要策略：

&emsp;*策略1.* **用$1\times 1$滤波器替换$3\times 3$滤波器.**&emsp;给定一定数量卷积滤波器的预算，我们将选择使这些滤波器中的大多数为$1\times 1$，因为$1\times 1$滤波器的参数比$3\times 3$滤波器少9倍。

&emsp;*策略2.* **减少$3\times 3$滤波器的输入通道数.**&emsp;考虑一个完全由$3\times 3$滤波器组成的卷积层。该层的参数总数为(输入通道数)\*(滤波器数)\*(3\*3)。因此，为了在CNN中保持一个小的参数总数，不仅要减少$3\times 3$滤波器的数量（见上面的策略1），而且也要减少$3\times 3$滤波器的输入通道数。我们使用squeeze层减少$3\times 3$滤波器的输入通道数，这将在下一节中描述。

&emsp;*策略3.* **在网络的后期下采样，这样卷积层就有了大的激活图.**&emsp;在卷积网络中，每个卷积层生成一个输出激活图，其空间分辨率至少为1x1，并且通常比1x1大得多。这些激活图的高度和宽度由以下因素控制：（1）输入数据的大小（如256x256大小的图像）和（2）在CNN架构中向下采样的层的选择。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-09-02-Xception--利用深度可分离卷积的深度学习/figure1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图1. 规范的Inception模块（Inception V3）</div>
</center>

## 4. 实验

&emsp;

### 4.1 JFT数据集

&emsp;

&emsp;

### 4.2 优化配置

&emsp;

&emsp;

&emsp;

## 4.4 训练基础设施

&emsp;

### 4.5 与Inception V3的比较

#### 4.5.1 分类性能

&emsp;

&emsp;

&emsp;

#### 4.5.2 大小和速度

&emsp;

### 4.6 残差连接的效果

&emsp;

&emsp;

### 4.7 逐点卷积后的一个中间激活的效果

&emsp;

## 5. 未来方向

&emsp;

## 6. 结论

&emsp;

---

## 个人看法

&emsp;这篇文章是Google对其之前的Inception系列所做的改进，通过解耦通道间的相关性与空间相关性，实现网络参数量的降低，这其实也是深度可分离卷积的根本思想所在，只不过Xception所采用的的深度卷积和逐点卷积的顺序以及采用激活函数的策略有所不同。
