---
layout:     post
title:      Going Deeper with Convolutions
subtitle:   GoogLeNet/Inception V1
date:       2019-09-24
author:     CJR
header-img: img/2019-09-24-InceptionV1/post-bg.jpg
catalog: true
mathjax: true
tags:
    - GoogLeNet
    - Inception
    - CNN
---

## GoogLeNet/Inception V1

&emsp;这篇文章是Inception系列的第一篇，提出了GoogleNet也就是Inception V1，文章发表在了CVPR 2015。原文可见[Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)。

---

## 摘要

&emsp;在ImageNet大型视觉识别挑战2014（ILSVRC14）中，我们提出了一种代号为Inception的深度卷积神经网络架构，实现了分类和检测的SOTA。该体系结构的主要特点是提高了网络内部计算资源的利用率。通过精心设计，我们增加了网络的深度和宽度，同时保持计算预算不变。为了优化质量，架构决策基于Hebbian原则和多尺度处理的直觉。在我们提交到ILSVRC14中使用的一种特殊形式是GoogLeNet，它是一个22层的深层网络，其质量是在分类和检测的背景下评估的。

## 1. 引言

&emsp;在过去的三年中，由于深度学习和卷积网络的发展，我们的目标分类和检测能力有了显著的提高。一个令人鼓舞的消息是，大部分的进展不仅仅是更强大的硬件、更大的数据集和更大的模型的结果，而且主要是新思想、算法和改进的网络架构的结果。例如，在ILSVRC 2014竞赛中，除了使用同一竞赛的分类数据集进行检测外，并没有使用新的数据源。我们提交给ILSVRC 2014的GoogLeNet实际上使用的参数是Krizhevsky等人两年前获奖架构\[AlexNet\]的12倍，同时明显更准确了。在目标检测方面，最大的收获并不是来自越来越大的深层网络的单纯应用，而是来自深层结构和经典计算机视觉的协同作用，如Girshick等人的R-CNN算法。

&emsp;另一个值得注意的因素是，随着移动和嵌入式计算的不断发展，我们算法的效率——尤其是它们的能力和内存使用——变得越来越重要。值得注意的是，本文提出的深层体系架构设计考虑因素包括了这一因素，而不是单纯地关注精度数字。在大多数实验中，这些模型的设计是为了在推理时保持15亿乘加法的计算预算，这样它们就不会成为纯粹的学术好奇心，而是可以以合理的成本投入到现实世界中，即使是在大型数据集上。

&emsp;在本文中，我们将专注于代号为Inception的高效的用于计算机视觉的深度神经网络体系结构，该结构的名称源自Lin等人在[Network in network]论文中的网络以及著名的“我们需要更深入”的网络模因。在我们的例子中，“deep”一词有两种不同的含义：首先，我们以“Inception模块”的形式引入了一个新的组织层次，同时也增加了网络深度。一般来说，我们可以把Inception模型看作是[Network in network]的逻辑顶点，同时从Arora等人的理论工作中获得灵感和指导。该架构的优点在ILSVRC 2014分类和检测挑战中得到了实验验证，其性能显著优于当前的SOTA。

## 2. 相关工作

&emsp;从LeNet-5开始，卷积神经网络（CNN）通常有一个标准的结构-堆叠卷积层（可选地随后是对比度归一化和最大池），之后是一个或多个全连接层。这种基本设计的变体在图像分类文献中普遍存在，并在MNIST、CIFAR和ImageNet分类挑战中获得了迄今为止最好的结果。对于较大的数据集，如Imagenet，最近的趋势是增加层数和层大小，同时使用dropout来解决过拟合问题。

&emsp;尽管担心最大池化层会导致精确空间信息的丢失，但与\[AlexNet\]相同的卷积网络架构也成功地用于定位、目标检测和人体姿态估计。

&emsp;Serre等人受灵长类视觉皮层神经科学模型的启发，使用一系列不同大小的固定Gabor过滤器来处理多个尺度。我们在这里使用了类似的策略。然而，与[Robust object recognition with cortex-like mechanisms]固定的2层深度模型相反，Inception架构中的所有滤波器都是学习的。此外，Inception层会重复很多次，导致在GoogLeNet模型中出现22层的深度模型。

&emsp;Network-in-Network是Lin等人为了提高神经网络的表征能力而提出的一种方法。在他们的模型中，额外增加了$1\times 1$卷积层，增加了网络的深度。我们在体系结构中大量使用这种方法。然而，在我们的设置中，1×1卷积有双重目的:最关键的是，它们主要用作降维模块来消除计算瓶颈，否则其将限制网络的大小。这不仅可以增加网络的深度，还可以在不影响性能的情况下增加网络的宽度。

&emsp;最后，当前的目标检测技术的SOTA是Girshick等人提出的基于卷积神经网络的区域检测方法（R-CNN）。R-CNN将整个检测问题分解为两个子问题：利用颜色和纹理等低层线索，以一种与类别无关的方式生成目标位置建议，并使用CNN分类器在这些位置识别目标类别。这种两阶段的方法利用了具有低层线索的边界框分割的准确性，以及最先进的CNNs的强大分类能力。我们在检测任务中采用了类似的过程，但是在这两个阶段都进行了增强，比如针对更高的目标边界框召回率的多框预测，以及集成方法用于更好地对边界框建议进行分类。

## 3. 动机和高层次的考虑

&emsp;提高深度神经网络性能最直接的方法是增加其规模。这包括增加深度（网络层次的数量）和宽度（每个层次的单元数量）。这是一种训练高质量模型的简单和安全的方法，特别是考虑到有大量标记的训练数据可用。然而，这个简单的解决方案有两个主要缺点。

&emsp;较大的尺寸通常意味着更多的参数，这使得扩大后的网络更容易发生过拟合，特别是当训练集中标记的例子数量有限时。这是一个主要的瓶颈，因为强标记数据集的获取既费力又昂贵，常常需要专业的人工评分员来区分各种细粒度的可视类别，比如ImageNet中的那些（即使是在1000类ILSVRC子集中），如图1所示。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-09-24-InceptionV1/figure1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图1. ILSVRC 2014分类挑战的1000个类别中的两个不同的类别。需要领域知识来区分这些类。</div>
</center>

&emsp;网络规模一致增长的另一个缺点是计算资源的使用显著增加。例如，在深度视觉网络中，如果两个卷积层被链接，它们的滤波器数量的任何均匀增加都会导致计算量的二次增长。如果没有有效地使用增加的容量（例如，如果大多数权重最终接近于零），那么大部分计算就会浪费。由于计算预算总是有限的，即使主要目标是提高性能的质量，有效地分配计算资源也比不加区别地增加规模好。

&emsp;解决这两个问题的一个基本方法是引入稀疏性，用稀疏层替换完全连接的层，甚至在卷积内部也是如此。除了模仿生物系统，由于Arora等人的开创性工作，这也将具有更坚实的理论基础的优势。他们的主要结果表明，如果数据集的概率分布可以用一个很大的、非常稀疏的深度神经网络来表示，那么通过分析前一层激活的相关统计量和聚类输出高度相关的神经元，可以逐层构建最优网络拓扑。尽管严格的数学证明需要非常强的条件，但事实上，这一说法与众所周知的Hebbian理论（一起激发的神经元连在一起）产生了共鸣，这表明，即使在不太严格的条件下，在实践中，基本思想也是适用的。

&emsp;不幸的是，当涉及到非均匀稀疏数据结构的数值计算时，当今的计算基础设施非常低效。即使算术运算的数量减少了100倍，查找和缓存未命中的开销也将占主导地位：切换到稀疏矩阵可能不会有回报。通过使用稳定改进和经过高度调整的数值库，利用底层CPU或GPU硬件的微小细节，可以实现极快的密集矩阵乘法，从而进一步扩大了差距。此外，非均匀稀疏模型需要更复杂的工程和计算基础设施。目前大多数面向视觉的机器学习系统仅通过卷积来利用空间域的稀疏性。然而，卷积是作为到较早层中patch的密集连接的集合来实现的。ConvNets自\[LeNet-5\]以来一直在特征维上使用随机稀疏连接表来打破对称性，提高学习能力，但为了进一步优化并行计算，趋势又回到了\[AlexNet\]的全连接。目前最先进的计算机视觉体系结构具有统一的结构。更多的滤波器和更大的批处理大小允许高效地使用密集的计算。

&emsp;这就提出了一个问题，是否还有希望实现下一个中间步骤：像理论所建议的那样，使用滤波器级别的稀疏性，但通过使用密集矩阵上的计算来利用我们当前的硬件的体系结构。关于稀疏矩阵计算的大量文献表明，将稀疏矩阵聚类成相对密集的子矩阵，往往会使稀疏矩阵乘法具有竞争性能。认为在不久的将来，类似的方法将被用于非统一的深度学习体系结构的自动化构建，似乎并不牵强。

&emsp;Inception体系结构最初是作为一个案例研究来评估复杂网络拓扑构造算法的假设输出的，该算法试图逼近[Provable bounds for learning some deep representations]所暗示的视觉网络的稀疏结构，并通过密集的，易于获得的组件来覆盖假设的结果。尽管这是一个高度投机的工作，但与基于[Network in network]的参考网络相比，在早期观察到了适度的收益。稍加调整，差距就扩大了，作为[Rich feature hierarchies for accurate object detection and semantic segmentation]和[Scalable object detection using deep neural networks]的基础网络，Inception在定位和目标检测的背景下被证明特别有用。有趣的是，虽然大多数最初的架构选择都在分离中受到了彻底的质疑和测试，但是它们在接近局部最优。不过，我们必须谨慎：尽管Inception架构已经成为计算机视觉领域的一个成功案例，但这是否可以归因于导致其构建的指导原则，仍然是个疑问。确保这一点将需要更彻底的分析和验证。

## 4. 架构细节

&emsp;Inception架构的主要思想是考虑卷积视觉网络的最优局部稀疏结构是如何被现成的密集组件逼近和覆盖的。注意，假设平移不变性意味着我们的网络将由卷积构建块构建。我们所需要的是找到最优的局部结构，并在空间上重复它。Arora等人提出了一种分层结构，在这种结构中，我们应该分析最后一层的相关统计数据，并将其聚类为具有高相关性的单元组。这些聚类构成下一层的单元，并与上一层中的单元连接。我们假设来自先前层的每个单元对应于输入图像的某个区域，这些单元被分组到滤波器组中。在较低的层（靠近输入的层）中，相关单元将集中在局部区域。因此，我们最终会得到很多聚类，它们集中在一个区域中，并且可以在下一层中被$1\times 1$卷积层覆盖，如[Network in network]中所建议的。然而，我们也可以料想到，在更大的patch上，能够被卷积覆盖的空间分布更广的聚类的数量会更少，而在越来越大的区域上，patch的数量会越来越少。为了避免patch对齐问题，当前的Inception体系结构被限制在$1\times 1$、$3\times 3$和$5\times 5$的滤波器大小；这个决定更多地是基于方便而不是必要性。这还意味着，建议的体系结构是所有这些层的组合，它们的输出过滤器组连接成一个输出向量，形成下一阶段的输入。此外，由于池化操作对于当前卷积网络的成功至关重要，因此建议在每个这样的阶段添加一个可选的并行池化路径也应该有额外的好处（参见图2（a））。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-09-24-InceptionV1/figure2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图2. Inception模块</div>
</center>

&emsp;由于这些“Inception模块”彼此堆叠在一起，它们的输出相关统计数据必然会有所不同：随着更高抽象的特性被更高的层捕获，它们的空间集中度预计会降低。这说明当我们移动到更高的层时，$3\times 3$和$5\times 5$的卷积的比例应该增加。

&emsp;上述模块的一个大问题是，至少在这种简单的形式下，即使是少量的5×5卷积，在具有大量滤波器的卷积层上也可能非常昂贵。一旦将池化单元添加到组合中，这个问题就变得更加突出：输出滤波器的数量等于前一阶段的滤波器数量。池化层的输出与卷积层的输出合并，将不可避免地导致输出数量逐级增加。虽然这种体系结构可能覆盖了最优的稀疏结构，但它的效率非常低，导致在几个阶段内计算量激增。

&emsp;这就引出了Inception架构的第二个想法：在计算需求增加过多的地方明智地减少维度。这是基于嵌入的成功：即使是低维嵌入也可能包含大量关于相对较大的图像patch的信息。然而，嵌入信息以密集、压缩的形式表示信息，压缩信息更难处理。在大多数地方，表示应该保持稀疏（根据[Provable bounds for learning some deep representations]的条件所要求的），并且仅在必须集中信号时才压缩它们。也就是说，在昂贵的$3\times 3$和$5\times 5$卷积之前，使用$1\times 1$卷积减少计算量。除了用于缩减计算量，它们还包括使用校正线性激活使它们具有双重用途。最终结果如图2（b）所示。

&emsp;通常，Inception网络是由上述类型的模块组成的网络，这些模块相互堆叠，偶尔使用带有stride为2的最大池化层来将网格的分辨率减半。出于技术原因（训练期间的内存效率），开始仅在较高层使用Inception模块，同时保持较低层的传统卷积方式似乎是有益的。这不是严格必要的，只是反映了我们当前实施中的一些基础设施的效率低下。

&emsp;这种体系结构的一个有用的方面是，它允许在每个阶段显著增加单元的数量，而不会在后期阶段造成计算复杂性的失控膨胀。这是通过在昂贵的大尺寸卷积之前普遍使用降维来实现的。此外，该设计遵循了视觉信息在不同尺度下处理和聚合的实践直觉，使下一阶段可以同时从不同尺度上抽象特征。

&emsp;计算资源的改进使用允许在不陷入计算困难的情况下增加每个阶段的宽度和阶段的数量。您可以利用Inception架构来创建稍差但计算上更便宜的版本。我们发现，所有可用的旋钮和杠杆（knobs and levers）都可实现计算资源的受控平衡，从而使网络的运行速度比具有非Inception架构的类似性能的网络快3到10倍，但是这时需要仔细的手动设计。

## 5. 实验

&emsp;

&emsp;

&emsp;

&emsp;

&emsp;

### 5.1 Cifar10

&emsp;

### 5.2 Street View House Numbers

&emsp;

### 5.3 German Traffic Sign Recognition Benchmark

&emsp;

## 6. 较

&emsp;

### 6.1 体系结构

&emsp;

&emsp;

### 6.2 调整

&emsp;

&emsp;

&emsp;

&emsp;

### 6.3 实验

&emsp;

&emsp;

## 7. 结论

&emsp;

---

## 个人看法

&emsp;
