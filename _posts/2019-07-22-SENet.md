---
layout:     post
title:      SENet
subtitle:   Squeeze-and-Excitation Networks
date:       2019-07-22
author:     CJR
header-img: img/2019-07-22-SENet/post-bg.jpg
catalog: true
mathjax: true
tags:
    - SENet
    - Image Classificatin
    - Lightweight Network
---

# SENet
这篇文章“Squeeze-and-Excitation Networks”是CVPR2018的一篇论文，因为最近看的MnasNet的论文里面提到了SENet里面提出的一个很有用的block，所以又翻出来看了下这篇文章，原论文见 [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)。

&emsp;下面是对论文的一个简单翻译：

---

## 摘要
&emsp;卷积神经网络（CNN）的核心组成部分是卷积算子，它通过在每一层的局部感受野内融合空间和通道信息，使网络能够构建信息特征。有很多以前的研究已经调查了这一关系的空间组成部分，寻求通过提高整个特征层次的空间编码质量来加强CNN的表征能力。在这项工作中，我们将重点放在通道关系上，并提出了一个新的架构单元，我们称之为“挤压和激励”（SE）块，它通过明确建模通道之间的相互依赖性来自适应地重新校准通道特征响应。我们表明，这些块可以堆叠在一起，形成SENET架构，可以跨不同数据集非常有效地进行泛化。我们进一步证明，SE块为现有最先进的CNN带来了显著的性能改进，而只需稍微增加计算成本。SENet是我们ILVRC 2017分类比赛提交的基础网络，它赢得了第一位，并将top-5错误率降低到了2.251%，超过了2016的获胜网络，相对提高了25%。模型和代码可从<https://github.com/hujie-frank/senet>获得。

## 1. 介绍
&emsp;卷积神经网络（CNN）已经被证明是解决各种视觉任务的有用模型。在网络的每个卷积层上，一组滤波器沿着输入通道表达邻域空间连接模式——在局部感受野内融合空间信息和通道信息。通过将一系列卷积层与非线性激活函数和下采样算子交叉，CNN能够生成图像表征，捕捉层次模式并获得全局理论感受野。计算机视觉研究的一个中心主题是寻找更强大的表示方法，只捕捉图像中对给定任务最显著的属性，从而提高性能。作为一种广泛应用于视觉任务的模型家族，新的神经网络体系结构设计的发展代表了这一研究的一个关键前沿。最近的研究表明，通过将学习机制集成到网络中，帮助捕获特征之间的空间相关性，可以增强CNNs产生的表征。其中一种方法是由Inception系列架构流行起来的，它将多尺度的过程合并到网络模块中，以提高性能。进一步的工作是寻求更好地对空间依赖关系进行建模，并将空间注意力纳入网络结构中。

&emsp;在本文中，我们研究了网络设计的另一个方面——通道之间的关系。我们引入了一个新的体系结构单元，我们称之为挤压-激励(SE)块，其目标是通过显式地建模卷积特征的通道之间相互依赖关系来提高网络产生的表征质量。为此，我们提出一种机制，允许网络执行特征重校准，通过该机制，它可以学习使用全局信息，选择性地强调信息特征，并抑制不太有用的特征。

&emsp;SE构建块的结构如图1所示。对于任意给定的变换$F_{tr}$（例如卷积）将输入$X$映射到特征图$U$，其中$U\in \mathbb{R}^{H\times W\times C}$，我们可以构造一个对应的SE块来进行特征重校准。特征$U$首先通过挤压（Squeeze）操作传递，该操作通过在空间维度($H\times W$)上聚合特征图来生成通道描述符。该描述符的功能是生成一个嵌入通道特征响应的全局分布，允许来自网络的全局感受野的信息被它的所有层使用。挤压之后是激励（excitation）操作，该操作采用简单的自选门机制的形式，以嵌入作为输入，并产生每个通道调制权值的集合。这些权值被应用到特征图$U$中，生成SE块的输出，可以直接输入到网络的后续层中。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-22-SENet/figure1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图1. SE块</div>
</center>

&emsp;设计和开发新的CNN架构是一项困难的工程任务，通常需要选择许多新的超参数和层配置。相反，SE块的结构很简单，可以直接在现有的最先进的体系结构中使用，用SE组件替换组件，从而有效地提高性能。SE块在计算上也是轻量级的，只会稍微增加模型的复杂性和计算负担。

&emsp;为了给这些主张提供证据，我们开发了几个SENets，并在ImageNet数据集上进行了广泛的评估。我们还提供了ImageNet之外的结果，这些结果表明我们的方法的好处并不局限于特定的数据集或任务。利用SENets，我们在ILSVRC 2017年分类竞赛中排名第一。我们的最佳模型集成在测试集上获得了2.251%的top-5误差。与前一年的最佳模型（top-5误差为2.991%）相比，这大约有25%的相对改进。

## 2. 相关工作
&emsp;**更深层次的体系结构**。VGGNets和Inception模型表明，增加网络的深度可以显著提高它能够学习的表征质量。通过调节各层输入的分布，批归一化(BN)为深度网络的学习过程增加了稳定性，并产生了更平滑的优化曲面。在这些工作的基础上，ResNets证明，通过使用基于恒等映射的跳层连接，可以学习更深、更强大的网络。Highway Networks引入了一种门控机制以调节沿捷径连接的信息流。在这些工作之后，对网络层之间的连接进行了进一步的重新定义，这显示了对深层网络的学习和表征特性的有希望的改进。

&emsp;另一种与此密切相关的研究方向是改进网络中包含的计算元素的功能形式的方法。 分组卷积已被证明是一种流行的方法，用于增加学习转换的基数。多分支卷积可以实现更灵活的算子组合，可以看作是分组算子的自然扩展。在以往的工作中，通常将跨通道相关性映射为特征的新组合，或者独立于空间结构，或者使用1×1卷积的标准卷积滤波器联合映射。这方面的研究主要集中在减少模型和计算复杂性的目标上，反映了一种假设，即可以将通道关系表示为具有局部感受野的实例不可知函数的组合。相反，我们提出，为单元提供使用全局信息显式地建模通道之间动态、非线性依赖关系的机制可以简化学习过程，并显著增强网络的表示能力。

&emsp;**算法架构搜索**。除了上述工作，还有一个丰富的研究历史，旨在放弃手工架构设计，转而寻求自动学习网络的结构。该领域的早期工作大多是在神经进化社区中进行的，该社区建立了使用进化方法搜索网络拓扑的方法。进化搜索虽然常常需要大量计算，但已经取得了显著的成功，包括为序列模型找到良好的记忆单元，以及为大规模图像分类学习复杂的体系结构。为了减少这些方法的计算量，提出了基于Lamarckian继承和可微体系结构搜索的高效替代方法。

&emsp;通过将架构搜索定义为超参数优化问题，随机搜索和其他更复杂的基于模型的优化技术也可以用来解决这个问题。拓扑选择作为可能的设计结构和直接体系结构预测的一条路径，已经被提出作为额外的可行的架构搜索工具。强化学习技术取得了特别显著的成果。SE块可以作为这些搜索算法的原子构建块，并在并行工作[MnasNet]中被证明是非常有效的。

&emsp;**注意力和门控机制**。注意力可以解释为将可用的计算资源分配给信号中信息量最大的部分的一种方法。注意力机制已经在许多任务中证明了它们的效用，包括序列学习、图像中的定位和理解、图像字幕和唇读。在这些应用中，它可以作为一个操作符合并到一个或多个表示更高级别抽象的层下面，以适应各种模式。一些工作提供了有趣的研究，结合使用空间和通道注意力。Wang等人提出了一种强大的基于沙漏模块的trunk-and-mask注意机制，该机制插入到深度残差网络的中间阶段。相比之下，我们提出的SE块包含一个轻量级的门控机制，其通过以计算高效的方式建模通道关系来增强网络的表征能力。

## 3. SE块

&emsp;SE块是一个计算单元，可以建立在将输入$X\in\mathbb{R}^{H'\times W'\times C'}$映射到特征图$U\in\mathbb{R}^{H\times W\times C}$的变换$F_{tr}$上。在接下来的符号中，我们把$F_{tr}$看作卷积运算符，并使用$V=[v_{1},v_{2},\ldots,v_{C}]$表示所学习的滤波核集，其中$v_{c}$表示第c个滤波器的参数。然后我们可以写出输出$U=[u_{1},u_{2},\ldots,u_{C}]$，其中

$$u_{c}=V_{C}*X=\sum_{s=1}^{C'}v_{c}^{s}*x^{s}$$

&emsp;这里$*$代表卷积，$v_{c}=[v_{c}^{1},v_{c}^{2},\ldots,v_{c}^{C'}]$，$X=[x^{1},x^{2},\ldots,x^{C'}]$且$u_{c}\in\mathbb{R}^{H}\times W$。$v_{c}^{s}$是一个二维空间核，表示$v_{c}$的一个单通道，作用于$X$对应的通道上。为了简化符号，省略了偏置项。由于输出是通过对所有通道的求和产生的，所以通道依赖关系隐式地嵌入到$v_{c}$中，但是与过滤器捕获的局部空间相关性纠缠在一起。卷积建模的通道关系本质上是隐式的和局部的（除了最顶层的那些）。我们期望通过显式地对通道相互依赖关系建模来增强对卷积特性的学习，从而使网络能够提高其对信息特性的敏感性，这些信息特性可以通过后续的转换加以利用。因此，我们希望为它提供对全局信息的访问，并在它们被送入下一个转换之前分两步重新校准过滤器响应，即挤压和激励。图1为SE块结构示意图。

### 3.1 Squeeze：全局信息嵌入

&emsp;为了解决利用通道依赖性的问题，我们首先在输出特性中考虑每个通道的信号。每个学习滤波器都只有一个局部感受野，因此转换输出$U$的每个单元都不能利用该区域之外的上下文信息。

&emsp;为了解决这个问题，我们建议将全局空间信息压缩到通道描述符中。这是通过使用全局平均池来生成通道级统计数据来实现的。形式上，统计量$z\in\mathbb{R}^{C}$是由$U$通过其空间维$H\times W$进行收缩生成的，使得$z$的第$c$个元素由下式计算得到：

$$ z_{c}=F_{sq}(u_{c})=\frac{1}{H\times W}\sum_{i=1}^{H}\sum_{j=1}^{W}u_{c}(i,j) $$

&emsp;*讨论*。转换的输出$U$可以解释为局部描述符的集合，这些局部描述符的统计信息表示整个图像。利用这些信息在以往的特征工程工作中很普遍。我们选择了最简单的聚合技术，全局池化，注意到这里也可以使用更复杂的策略。

### 3.2 Excitation：自适应再校准

&emsp;为了利用在挤压操作中聚合的信息，我们在此之后进行了第二个操作，其目的是完全捕获通道依赖关系。为了实现这一目标，函数必须满足两个标准：第一，它必须是灵活的（特别是它必须能够学习通道之间的非线性相互作用），第二，它必须学习一种非互斥关系，因为我们希望确保多个通道可以被强调（而不是强制只有一个激活（one-hot））。为了满足这些条件，我们选择使用一个简单的使用sigmoid激活函数的门控机制:

$$
s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_{2}\delta(W_{1}z))
$$


&emsp;其中$\delta$是指ReLU函数，$W_{1}\in\mathbb{R}^{\frac{C}{r}\times C}$且$W_{1}\in\mathbb{R}^{C\times\frac{C}{r}}$。为了限制模型的复杂性并有助于泛化，我们通过在非线性周围形成一个具有两个完全连接层的瓶颈来参数化门控机制，即具有缩减率$r$的维度缩减层（该参数选择在第6.1节中讨论）、ReLU和一个维数增加层返回到变换输出$U$的通道维数。块的最终输出是通过使用激活$s$重新缩放$U$获得的：

$$
\tilde{\mathbf{x}}_{c}=\mathbf{F}_{scale}(\mathbf{u}_{c},s_{c})=s_{c}\mathbf{u}_{c}
$$

&emsp;其中$\tilde{X}=[\tilde{x}_{1},\tilde{x}_{2},\ldots,\tilde{x}_{C}]$，$\mathbf{F}_{scale}(\mathbf{u}_{c}, s_{c})$为标量$s_{c}$与特征图$\mathbf{u}_{c}\in\mathbb{R}^{H\times W}$之间的通道相乘。

&emsp;*讨论*。激励算子将特定于输入的描述符$z$映射到一组通道权值。在这一点上，SE块本质上引入了以输入为条件的动态，它可以被视为通道上的一个自注意函数，这些通道的关系不局限于卷积滤波器响应的局部感受野。

### 3.3 实例化

## 4. 移动神经结构搜索

&emsp;在这一部分，我们将首先讨论我们提出的新的分解层次搜索空间，然后总结我们基于强化学习的搜索算法。

### 4.1 分解的层次搜索空间

&emsp;最近的研究表明一个定义良好的搜索空间对于神经结构搜索是非常重要的。然而，大多数以前的方法只搜索一些复杂的单元格，然后重复堆栈相同的单元格。这些方法不允许层多样性，但这对于实现高精度和低延迟都是至关重要的。

&emsp;与之前的方法不同，我们引入了一种新的分解层次搜索空间，它将CNN模型分解为独特的块，然后分别搜索每个块的操作和连接，从而允许在不同块中使用不同的层结构。我们的直觉是，我们需要根据输入和输出形状搜索最佳操作，以获得更好的精度-延迟权衡。例如，CNNs的早期阶段通常处理更大量的数据，因此对推理延迟的影响比后期阶段大得多。形式上，考虑一个表示为一个四元组$(K,K,M,N)$的广泛使用的深度可分离卷积内核，它将大小为$(H,W,M)$的输入转换为大小为$(H,W,N)$的输出，其中$(H,W)$是输入分辨率，$M$，$N$是输入/输出滤波器尺寸。乘法-加法的总数可以描述为:

$$ H*W*M*(K*K+N) $$

&emsp;在这里，如果总计算量受到限制，我们需要小心地平衡内核大小$K$和滤波器尺寸$N$。例如，增大某一层的原本大小为$K$的核以增加感受野，必须减少同一层的滤波器大小$N$或从其他层计算得到的结果以保持平衡。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/figure4.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图4. 分解的层次搜索空间。 根据网络层的输入分辨率和滤波器尺寸，将它们分组为若干预定义的框架(称为块)。每个块包含一个可变数量的重复相同的层，其中只有第一层具有步长2（如果输入/输出分辨率不同），但所有其他层都具有步长1。对于每个块，我们搜索一个单层的操作和连接以及层数N，然后重复N次相同的层(例如，Layer 4-1到Layer 4-N4是相同的)。来自不同块的层(例如，Layer 2-1和Layer 4-1)可以是不同的。</div>
</center>

&emsp;图4显示了搜索空间的基线结构。我们将CNN模型划分为一系列预定义的块，逐渐降低输入分辨率，并像许多CNN模型一样增加滤波器尺寸。每个块都有一组相同的层，它们的操作和连接由每个块子搜索空间决定。具体来说，块$i$的子搜索空间由以下选项组成：

- 卷积运算操作$ConvOp$：常规卷积 (conv)、深度卷积(dconv)、移动倒瓶颈卷积
- 卷积核大小$KernelSize$：3x3，5x5
- 挤压-激励比$SERatio$：0，0.25
- skip操作$SkipOp$：池化，单位残差（identity residual），或没有skip
- 输出滤波器尺寸$F_{i}$
- 每个块的层数$N_{i}$

&emsp;$ConvOp$、$KernelSize$、$SERatio$、$SkipOp$、$F_{i}$决定一个层的架构，而$N_{i}$决定该层将在块中重复多少次。例如，图4中块4的每一层都有一个反向瓶颈5x5卷积和一个单位残差跳转路径，同一层重复$N_{4}$次。 我们使用MobileNetV2作为参考对所有搜索选项进行离散化：对于每个块中的层数，我们基于MobileNetV2搜索$\{0，+1，-1\}$；对于每层的滤波器尺寸，我们搜索它关于MobileNetV2的相对尺寸$\{0.75,1.0,1.25\}$。

&emsp;我们的分层分解搜索空间在平衡层次的多样性和搜索空间的大小方面具有明显的优势。假设我们将网络划分为$B$个块，每个块都有一个大小为$S$的子搜索空间，每个块平均有$N$层，那么我们的总搜索空间大小将是$S^{B}$，相对于大小为$S^{B*N}$的平坦逐层搜索空间。一个典型的例子是$S = 432$，$B = 5$，$N = 3$，那么我们的搜索空间大小约为$10^{13}$，对比之下使用逐层搜索方法的空间大小为$10^{39}$。

### 4.2 搜索算法

&emsp;受近期工作的启发，我们使用强化学习方法为我们的多目标搜索问题找到Pareto最优解。我们选择强化学习是因为它很方便，而且奖励函数也很容易定制，但是我们认为其他方法，比如[evolution]也能奏效。

&emsp;具体来说，我们遵循与[Learning transferable architectures for scalable image recognition]相同的思想，将搜索空间中的每个CNN模型映射到令牌列表。这些令牌由强化学习代理基于参数$\theta$的一系列动作$a_{1:T}$确定。我们的目标是最大化预期的奖励：

$$J=E_{P(a_{1:T};\theta)}[R(m)]$$

&emsp;其中$m$为动作$a_{1:T}$确定的采样模型，$R(m)$为目标值。

&emsp;如图1所示，搜索框架由三个部分组成：一个基于递归神经网络(RNN)的控制器、一个获取模型精度的训练器和一个用于测量延迟的基于手机的推理引擎。我们遵循众所周知的sample-eval-update循环来训练控制器。在每个步骤中，控制器首先使用其当前参数$\theta$对一批模型进行采样，方法是根据其RNN中的SoftMax逻辑预测令牌序列。对于每一个采样的模型$m$，我们将其训练在目标任务上，得到其精度$ACC(m)$，并将其运行在真实的手机上，得到其推理延迟$LAT(m)$，然后计算奖励值$R(m)$。在每个步骤结束时，通过使用近端策略优化(Proximal Policy Optimization)最大化奖励的期望，从而更新控制器的参数$\theta$。sample-eval-update循环重复，直到达到最大迭代次数或参数$\theta$收敛。

## 5. 实验设置

&emsp;直接搜索像ImageNet或COCO这样的大型任务的CNN模型是昂贵的，因为每个模型需要几天的时间来收敛。虽然以前的方法主要在较小的任务上执行架构搜索，比如CIFAR-10，但是当考虑到模型延迟时，我们发现这些较小的代理任务不起作用，因为当应用于较大的问题时，通常需要扩大模型的规模。在本文中，我们直接在ImageNet训练集上执行架构搜索，但是只需要较少的训练步骤(5个epoch)。通常情况下，我们会从训练集中随机选取50K的图像作为固定的验证集。为了保证精度的提高是从我们的搜索空间中得到的，我们使用了与NASNet相同的RNN控制器，尽管它并不高效：在64台TPUv2设备上，每次架构搜索需要4.5天。在训练过程中，我们通过在Pixel 1手机的单线程大CPU内核上运行来测量每个采样模型的实际延时。总的来说，我们的控制器在架构搜索过程中采样了大约8K个模型，但是只有15个性能最好的模型被转移到完整的ImageNet，只有1个模型被转移到COCO。

&emsp;对于完整的ImageNet训练，我们使用RMSProp优化器，使用0.9的decay和0.9的momentum。在每个卷积层之后加入批归一化，momentum为0.99，weight decay为1e-5。最后一层应用0.2的Dropout。跟[Accurate, large minibatch sgd: training imagenet in 1 hour.]一样，学习率在前5个epoch从0增加到0.256，然后每2.4个epoch衰减0.97。我们使用batch大小4K和图像大小224×224的起始预处理。对于COCO的训练，我们将学习到的模型插入到SSD检测器中，使用与[MobileNetV2]相同的设置，包括输入大小为320×320。

## 6. 结果

&emsp;在本节中，我们研究了我们的模型在ImageNet分类和COCO目标检测方面的性能，并与其他最先进的移动模型进行了比较。

### 6.1 ImageNet分类性能

&emsp;表1显示了我们的模型在ImageNet上的性能。我们将目标延迟设置为$T=75ms$，类似于MobileNetV2，并在架构搜索期间使用$\alpha=\beta=-0.07$的奖励函数。然后，我们从相同的搜索实验中选择了三个性能最好的MnasNet模型，并将它们与现有的移动模型进行了比较。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/table1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表1. ImageNet分类的性能结果。我们将我们的MnasNet模型与手动设计的移动模型和其他自动化方法进行了比较——MnasNet-A1是我们的基线模型；MnasNet-A2和MnasNet-A3是两个来自同一架构搜索实验的具有不同延迟的模型（用于比较）；#Params：可训练参数量；#Mult-Adds：对于每张图像来说的乘法-加法运算数；Top-1/5 Acc：ImageNet验证集的top-1或top-5精度；Inference Latency：在批量大小为1的Pixel 1手机的大CPU核心上测量。</div>
</center>

&emsp;如表所示，我们的MnasNet A1模型达到了75.2%的top-1/92.5%的top-5精度，延迟为78ms，3.9M参数/312M乘加法运算量，为这种典型的移动延迟约束实现了一种新的最先进的精度。特别是，MnasNet在相同的Pixel手机上运行速度比MobileNetV2(1.4)快1.8倍，准确率高0.5%。与自动搜索CNN模型相比，我们的MnasNet运行速度比移动尺寸的NASNet-A快2.3倍，top-1准确率高1.2%。值得注意的是，我们的稍微大一点的MnasNet-A3模型比ResNet-50具有更好的精度，但是参数减少了4.8倍，乘加法成本减少了10倍。

&emsp;鉴于挤压和激励（Squeeze-and-excitation networks）相对较新，许多现有的移动模型没有这种额外的优化，我们也在表2中的搜索空间中显示了没有SE的搜索结果；我们的自动化方法仍然显著优于MobileNetv2和NASNet。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/table2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表2. 挤压和激发（SE，Squeeze-and-Excitation）的性能研究 – MnasNet-A表示搜索空间中带有SE的默认MnasNet；MnasNet-B表示搜索空间中没有SE的MnasNet。</div>
</center>

### 6.2 模型扩展性能

&emsp;考虑到现实世界中存在的无数应用程序需求和设备异构性，开发人员通常会将模型按比例放大或缩小，以牺牲准确性来换取延迟或模型大小。一种常用的缩放技术是使用深度乘数修改滤波器的尺寸。例如，深度乘数0.5将每个层中的通道数减半，从而减少了延迟和模型大小。另一种常用的缩放技术是在不改变网络的情况下减小输入图像的大小。

&emsp;图5通过改变深度乘数和输入图像大小，比较了MnasNet和MobileNetV2的模型缩放性能。当深度乘数从0.35变为1.4时，推理延迟也从20ms变为160ms。如图5a所示，对于每个深度乘数，我们的MnasNet模型的精度始终优于MobileNetV2。同样，我们的模型对输入大小的变化也很健壮，在96到224的所有输入图像大小中，它的性能始终优于MobileNetV2(精度提高了4.1%)，如图5b所示。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/figure5.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图5. 不同模型缩放技术的性能比较。MnasNet是我们的基线模型，如表1所示。我们使用与MobileNetV2相同的深度乘数和输入尺寸来缩放它。</div>
</center>

&emsp;除了模型扩展，我们的方法还允许为任何延迟目标搜索新的体系结构。例如，一些视频应用程序可能需要低至25毫秒的延迟。我们可以缩小基线模型的规模，或者搜索专门针对这种延迟约束的新模型。表4比较了这两种方法。为了公平比较，我们对所有模型使用相同的224x224大小图像。虽然我们的MnasNet在相同的缩放参数下已经优于MobileNetV2，但是我们可以通过针对22ms延迟约束的新架构搜索进一步提高精度。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/table4.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表4. 模型缩放 vs 模型搜索 – MobileNetV2（0.35x）和MnasNet-A1（0.35x）表示使用深度乘数0.35缩放基线模型；MnasNet-search1/2表示针对22ms延迟限制的新架构搜索的模型。</div>
</center>

### 6.3 COCO目标检测性能

&emsp;对于COCO对象检测，我们选取表2中的MnasNet模型作为SSDLite的特征提取器，这是SSD的一个改进的资源高效版本。与[MobileNetV2]类似，我们将我们的模型与其他移动尺寸的SSD或YOLO模型进行比较。

&emsp;表3显示了我们的MnasNet模型在COCO上的性能。YOLO和SSD的结果来自[Yolo9000]，MobileNets的结果来自[MobileNetV2]。我们在COCO trainval35k上训练我们的模型，并在test-dev2017上进行评估，将结果提交给COCO server。如表所示，我们的方法在MobileNet V1和V2上显著提高了精度。与标准的SSD300检测器相比，我们的MnasNet模型在7.4倍更少的参数和42倍更少的乘法-加法运算量的情况下，实现了与SSD300相当的mAP质量(23.0 vs 23.2)。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/table3.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表3. COCO目标检测性能结果 - #Params：可训练参数个数；#Mult-Adds：每张图像需要的乘法-加法运算量；mAP：test-dev2017上的标准平均精度；mAP_S、mAP_M、mAP_L:小、中、大目标的平均精度；推理延迟：Pixel 1手机上的推理延迟。</div>
</center>

## 7. 消融实验及讨论

&emsp;在本节中，我们将研究延迟约束和搜索空间的影响，并讨论MnasNet体系结构细节和层多样性的重要性。

### 7.1 软延迟与硬延迟约束

&emsp;多目标搜索方法允许我们通过在奖励方程中设置不同的$\alpha$和$\beta$值处理硬和软延迟约束。图6显示了典型的$\alpha$和$\beta$的多目标搜索结果。当$\alpha=0$，$\beta=1$时，延迟被当作一个硬约束，所以控制器往往更关注速度更快的模型,以避免延迟惩罚。另一方面，通过设置$\alpha=\beta=−0.07$，控制器将目标延迟作为一个软约束，并试图在更大的延迟范围内寻找模型。它在目标延迟值(75ms)附近采样了更多的模型，但也探索了延迟小于40ms或大于110ms的模型。这允许我们在单个架构搜索中从Pareto曲线中选择多个模型，如表1所示。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/figure6.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图6. 多目标搜索结果 (a)α=0，β=1； 和(b)α=β=−0.07。目标延迟为T=75ms。上图为3000个采样模型(绿点)的Pareto曲线(蓝线)；下图为模型延迟的直方图。</div>
</center>

### 7.2 理清搜索空间和奖励

&emsp;为了理清我们的两个关键贡献的影响：多目标奖励和新的搜索空间，表5比较了它们的性能。从NASNet开始，我们首先使用相同的单元基搜索空间，并使用我们提议的多对象奖励简单地添加延迟约束。结果表明，它通过将精度转换为延迟来生成更快的模型。然后，我们应用我们的多目标奖励和我们新的分解搜索空间，实现更高的准确性和更低的延迟，表明我们的搜索空间的有效性。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/table5.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表5. 解耦搜索空间与奖励设计的比较 - Multi-obj表示我们的多目标奖励；Single-obj仅表示优化精度。</div>
</center>

### 7.3 MnasNet体系结构和层多样性

&emsp;图7(a)说明了通过自动化方法找到的MnasNet-A1模型。正如预期的那样，它由各种层架构组成。一个有趣的观察是，我们的MnasNet同时使用了3x3和5x5卷积，这与之前的移动模型都只使用3x3卷积不同。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/figure7.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图7. MnasNet-A1体系结构 - (a)为表1中选取的代表性模型；(b)-(d)是一些对应的层结构。MBConv为移动倒瓶颈卷积，DWConv为深度卷积，k3x3/k5x5为核大小，BN为批归一化，HxWxF为张量形状(高、宽、深)，×1/2/3/4为块内重复层数。</div>
</center>

&emsp;为了研究层多样性的影响，表6比较了MnasNet及其只重复单一类型层的变体(固定内核大小和扩展率)。我们的MnasNet模型比那些变体具有更好的精度-延迟权衡，这突出了资源受限CNN模型中层多样性的重要性。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/table6.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">表6. MnasNet及其变体的性能比较 — MnasNet-A1表示图7(a)所示的模型；另一些变体在整个网络中重复单一类型的层。所有的模型在每一层都有相同的层数和相同的滤波器尺寸。</div>
</center>

## 8. 结论

&emsp;本文提出了一种利用强化学习设计资源高效的移动CNN模型的自动神经结构搜索方法。我们的主要想法是将平台感知的真实世界的延迟信息整合到搜索过程中，并利用一个新的因子化的层次搜索空间来搜索在准确性和延迟之间具有最佳权衡的移动模型。我们证明，我们的方法可以自动找到比现有方法明显更好的移动模型，并在典型的移动推理延迟约束下，在ImageNet分类和COCO目标检测方面实现新的最先进的结果。此产生的MnasNet架构还提供了关于层多样性重要性的有趣发现，这将指导我们设计和改进未来的移动CNN模型。
