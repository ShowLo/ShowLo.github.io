---
layout:     post
title:      MnasNet
subtitle:   Platform-Aware Neural Architecture Search for Mobile
date:       2019-07-17
author:     CJR
header-img: img/2019-07-17-MnasNet/post-bg.jpg
catalog: true
mathjax: true
tags:
    - MnasNet
    - Neural Network
    - AI
    - Deep Learning
    - Machine Learning
    - Lightweight Network
---

# MnasNet
这篇文章“MnasNet：移动平台感知神经结构搜索”是谷歌发表在CVPR2019的一篇论文，原论文见 [MnasNet: Platform-Aware Neural Architecture Search for Mobile](https://arxiv.org/abs/1807.11626?context=cs.LG)。

&emsp;下面是对论文的一个简单翻译：

---

## 摘要
&emsp;为移动设备设计卷积神经网络(CNN)是一个挑战，因为移动模型需要小而快，但仍然准确。尽管在各个方面都致力于设计和改进移动CNNs，但是当有如此多的架构可能性需要考虑时，手工平衡这些trade-offs是非常困难的。在本文中，我们提出了一种自动移动神经架构搜索(MNAS)方法，该方法将模型延迟显式地纳入到主要目标中，以便能够搜索出一个在精度和延迟之间实现良好权衡的模型。与之前的工作不同，之前延迟是通过另一个通常不太准确的指标(例如FLOPS)来考虑的，我们的方法直接通过在手机上执行模型来测量实际的推理延迟。为了进一步在灵活性和搜索空间大小之间取得适当的平衡，我们提出了一种新的分解层次搜索空间，它鼓励整个网络的层多样性。实验结果表明，我们的方法始终优于最先进的移动CNN模型在多个视觉任务上的表现。在ImageNet分类任务中，我们的MnasNet在Pixel手机上实现了75.2%的top-1准确率，延迟为78ms，比MobileNetV2快1.8倍，准确率提高0.5%，比NASNet快2.3倍，准确率提高1.2%。我们的MnasNet在COCO对象检测方面也比MobileNets具有更好的mAP。代码位于<https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet>。

## 1. 介绍
&emsp;卷积神经网络(CNN)在图像分类、目标检测和许多其他应用领域取得了显著的进展。随着现代CNN模型变得越来越深、越来越大，它们也变得越来越慢，需要更多的计算。计算需求的增加使得在例如移动设备或嵌入式设备之类的资源受限的平台上部署最先进的CNN模型变得困难。

&emsp;由于移动设备上可用的计算资源有限，最近的许多研究都集中在设计和改进移动CNN模型上，通过降低网络的深度和使用更便宜的操作，如深度卷积和组卷积。然而，设计一个资源受限的移动模型是具有挑战性的:一个人必须小心地平衡准确性和资源效率，导致一个显著大的设计空间。

&emsp;本文提出了一种用于移动CNN模型设计的自动神经结构搜索方法。图1显示了我们的方法的概述，其中与以前方法的主要区别是延迟感知多目标奖励和新的搜索空间。我们的方法基于两个主要思想。首先，我们将设计问题表示为一个多目标优化问题，同时考虑CNN模型的准确性和推理延迟。与以前使用FLOPS来近似推断延迟的工作不同，我们直接通过在实际移动设备上执行模型来测量实际的延迟。我们的想法的灵感来源于FLOPS往往是一个不准确的指标：例如MobileNet和NASNet有类似的FLOPS(575M与564M)，但是他们的延迟明显不同(113ms与183ms，细节见表1)。其次，我们观察到以前的自动化方法主要是搜索一些类型的单元，然后通过网络重复地堆叠相同的单元。这简化了搜索过程，但也排除了对计算效率很重要的层多样性。为了解决这个问题，我们提出了一种新的分解层次搜索空间，它允许层在架构上不同，但仍然在灵活性和搜索空间大小之间取得适当的平衡。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/figure1.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图1. 基于移动平台的神经网络搜索研究综述</div>
</center>

&emsp;我们将所提出的方法应用于ImageNet分类和COCO目标检测。图2总结了我们的MnasNet模型和其他最先进的移动模型之间的比较。与MobileNetV2相比，我们的模型提高了3.0%的ImageNet精度，在谷歌Pixel手机上具有类似的延迟。另一方面，如果我们限制目标精度，那么我们的MnasNet模型比MobileNetV2快1.8倍，比NASNet快2.3倍，具有更好的精度。与广泛使用的ResNet-50相比，我们的MnasNet模型在少4.8倍的参数和少10倍的乘法-加法操作的情况下，精度略高(76.7%)。通过将我们的模型作为特征提取器插入SSD对象检测框架，我们的模型在MobileNetsV1和MobileNetV2上改进了COCO数据集的推理延迟和mAP，并以少42倍的乘法-加法操作实现了与SSD300相当的mAP(23.0 vs 23.2)。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/figure2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图2. 准确性vs延迟——我们的MnasNet模型在ImageNet上明显优于其他移动模型。详情见表1</div>
</center>

&emsp;总而言之，我们的主要贡献如下:

&emsp;1. 我们介绍了一种多目标神经结构搜索方法，它可以优化移动设备上的准确性和实际延迟。

&emsp;2. 我们提出了一种新颖的分解分层搜索空间，以实现层的多样性，但仍然在灵活性和搜索空间大小之间取得了适当的平衡。

&emsp;3. 我们在典型的移动延迟限制下，在ImageNet分类和COCO对象检测上展示了STOA的精确度。

## 2. 相关工作
&emsp;近年来，提高CNN模型的资源效率一直是一个活跃的研究课题。一些常用的方法包括:1)将基线CNN模型的权重和/或激活量化为低比特表示，或2)根据FLOPs修剪不太重要的滤波器，或根据平台感知的指标(如[Netadapt]中引入的延迟)。然而，这些方法都是与基线模型相联系的，并且不专注于学习CNN操作的新组成部分。

&emsp;另一种常见的方法是直接手工制作更高效的移动架构：SqueezeNet通过使用更低成本的1x1卷积和减小滤波器尺寸，减少了参数量和计算量；MobileNet广泛采用深度可分离卷积来最小化计算密度；ShuffleNets利用低成本的组卷积和信道洗牌；Condensenet学习跨层连接组卷积；最近，MobileNetV2通过使用资源效率高的反向残差和线性瓶颈，在移动尺寸的模型中取得了最先进的结果。不幸的是，考虑到潜在的巨大设计空间，这些手工制作的模型通常需要大量的人力。

&emsp;近年来，利用神经结构搜索实现模型设计过程的自动化越来越受到人们的关注。这些方法主要基于强化学习、进化搜索、可微搜索等学习算法。虽然这些方法可以通过重复堆积一些搜索单元来生成移动尺寸的模型，但是它们并没有将移动平台约束合并到搜索过程或搜索空间中。与我们的工作密切相关的是MONAS、DPP-Net、RNAS和Pareto-NASH，它们在搜索CNNs时，试图优化多个目标，如模型大小和精度，但是它们的搜索过程只对CIFAR等小任务进行了优化。相比之下，本文针对的是实际的移动延迟约束，重点研究了ImageNet分类和COCO对象检测等较大的任务。

## 3. 问题公式化

&emsp;我们将设计问题表示为一个多目标搜索，目标是找到既具有高精度又具有低推理延迟的CNN模型。不像以前的架构搜索方法常常优化间接指标，比如FLOPS，我们考虑直接的真实世界的推理延迟，方法是在真实的移动设备上运行CNN模型，然后将真实世界的推理与我们的目标结合起来。这样做可以直接测量在实践中可以实现什么：我们早期的实验表明，由于移动硬件/软件特性的多样性，要近似真实世界的延迟是很有挑战性的。

&emsp;给定模型$m$，让$ACC(m)$表示其在目标任务上的精度，$LAT(m)$表示目标移动平台上的推理延迟，$T$表示目标延迟。一种常见的方法是将$T$视为硬约束，并在此约束下最大限度地提高精度：

$$ \mathop{maximize}\limits_{m}\quad ACC(m)$$

$$ subject\ to\quad LAT(m)\le T $$

&emsp;然而，这种方法只最大化了一个度量，并没有提供多个Pareto最优解。非正式地说，如果模型具有不增加延迟的最高精度，或者具有不降低精度的最低延迟，则称为Pareto最优。考虑到执行架构搜索的计算成本，我们更感兴趣的是在单个架构搜索中找到多个Pareto最优解。
虽然文献[Multi-objective optimization]中有很多方法，但我们使用自定义加权积方法近似Pareto最优解，优化目标定义为：

$$ \mathop{maximize}\limits_{m}\quad ACC(m)\times \left[\frac{LAT(m)}{T}\right]^{w} $$

&emsp;其中$w$为权重因子，定义为：

$$ w=\left\{
\begin{aligned}
&\alpha,\ if\ LAT(m) \le T\\
&\beta,\ otherwise 
\end{aligned}
\right.
$$

&emsp;$\alpha$和$\beta$是特定于应用程序的常数。挑选$\alpha$和$\beta$的经验法则是确保Pareto最优解决方案在不同的精度-延迟权衡下具有类似的奖励。例如，我们根据经验观察到，延迟加倍通常会带来5%的相对准确度增益。给出两种模型：(1)M1具有延时$l$和精度$a$；(2)M2具有延迟$2l$和5%更高的精度$a\cdot(1 + 5\%)$，他们应该也有类似的奖励：$Reward(M2)=a\cdot (1+5\%)\cdot(2l/T)^{\beta}\approx Reward(M1)=a\cdot (l/T)^{\beta}$。解得$\beta\approx −0.07$。 因此，我们在我们的实验中使用$\alpha=\beta=−0.07$，除非显式声明。


&emsp;图3显示了目标函数的两个典型值$(\alpha,\beta)$。在上面的图$(\alpha=0,\beta=−1)$，如果测量延迟小于目标延迟$T$，我们只需使用精度作为目标值；否则，我们将严格惩罚目标值，以阻止模型违反延迟约束。底部图$(\alpha=\beta=−0.07)$将目标延迟$T$视为一个软约束,并根据测量延迟平滑调整目标值。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://raw.githubusercontent.com/ShowLo/ShowLo.github.io/master/img/2019-07-17-MnasNet/figure3.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">图3. 目标函数，假设准确率ACC(m)=0.5，目标延迟T=80ms：上面的图表示以延迟为硬约束的目标值；下图的图显示了以延迟为软约束的目标值。</div>
</center>

## 4. 移动神经结构搜索

&emsp;在这一部分，我们将首先讨论我们提出的新的分解层次搜索空间，然后总结我们基于强化学习的搜索算法。

### 4.1 分解的层次搜索空间

&emsp;最近的研究表明一个定义良好的搜索空间对于神经结构搜索是非常重要的。然而，大多数以前的方法只搜索一些复杂的单元格，然后重复堆栈相同的单元格。这些方法不允许层多样性，但这对于实现高精度和低延迟都是至关重要的。

&emsp;与之前的方法不同，我们引入了一种新的分解层次搜索空间，它将CNN模型分解为独特的块，然后分别搜索每个块的操作和连接，从而允许在不同块中使用不同的层结构。我们的直觉是，我们需要根据输入和输出形状搜索最佳操作，以获得更好的精度-延迟权衡。例如，CNNs的早期阶段通常处理更大量的数据，因此对推理延迟的影响比后期阶段大得多。形式上，考虑一个表示为一个四元组$(K,K,M,N)$的广泛使用的深度可分离卷积内核，它将大小为$(H,W,M)$的输入转换为大小为$(H,W,N)$的输出，其中$(H,W)$是输入分辨率，$M$，$N$是输入/输出滤波器尺寸。乘法-加法的总数可以描述为:

$$ H*W*M*(K*K+N) $$

&emsp;在这里，如果总计算量受到限制，我们需要小心地平衡内核大小$K$和滤波器尺寸$N$。例如，增大某一层的原本大小为$K$的核以增加感受野，必须减少同一层的滤波器大小$N$或从其他层计算得到的结果以保持平衡。

&emsp;图4显示了搜索空间的基线结构。我们将CNN模型划分为一系列预定义的块，逐渐降低输入分辨率，并像许多CNN模型一样增加滤波器尺寸。每个块都有一组相同的层，它们的操作和连接由每个块子搜索空间决定。具体来说，块$i$的子搜索空间由以下选项组成：

- 卷积运算操作$ConvOp$:常规卷积 (conv)、深度卷积(dconv)、移动倒瓶颈卷积
- 卷积核大小$KernelSize$：3x3, 5x5
- 挤压-激励比$SERatio$：0,0.25
- skip操作$SkipOp$：池化，单位残差（identity residual），或没有skip
- 输出滤波器尺寸$F_{i}$
- 每个块的层数$N_{i}$

&emsp;$ConvOp$、$KernelSize$、$SERatio$、$SkipOp$、$F_{i}$决定一个层的架构，而$N_{i}$决定该层将为块重复多少次。例如，图4中块4的每一层都有一个反向瓶颈5x5卷积和一个单位残差跳转路径，同一层重复$N_{4}$次。 我们使用MobileNetV2作为参考对所有搜索选项进行离散化：对于每个块中的层数，我们基于MobileNetV2搜索${0，+1，-1}$；对于每层的滤波器尺寸，我们在中搜索它相对于MobileNetV2的相对尺寸${0.75,1.0,1.25}$。
